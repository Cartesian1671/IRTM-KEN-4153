{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cartesian1671/IRTM-KEN-4153/blob/main/01_IRTM_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by copying this into your Google Drive!!"
      ],
      "metadata": {
        "id": "3VgDWmVan1SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![maastricht-university-logo.jpeg](data:image/jpeg;base64,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)\n",
        "#Faculty of Science and Engineering - Department of Advanced Computer Sciences\n",
        "# Course Information Retrieval and Text Mining - Tutorial Tokenization"
      ],
      "metadata": {
        "id": "8S4ipRELHrMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Jan Scholtes\n",
        "\n",
        "\n",
        "Welcome to the tutorial on Tokenization. In this notebook you will learn how to preprocess text into tokens.\n",
        "\n",
        "This is the basis of any Information Retrieval, Text Mining or NLP process. Tokenization is closely related to sentence detection, stemming, lemmatization and is part of the large NLP research topic named morphology. \n",
        "\n",
        "Tokenization is highly language dependent. In this tutorial we focus on Western-European languages. \n",
        "\n",
        "In this notebook, we will use the Stanford NLTK library.\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZVnJSqfnp_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text extraction and cleanup is an important component of real-world NLP systems. Text extraction allows one to extract text from various electronic file formats (TXT, HTML, XML, PDF, DOCX, XLSX, PPTX, ...) and deals with the encoding of the characters (Unicode, UTF-8, Code pages or ACSII). \n",
        "\n",
        "Libraries such as BeautifulSoup, Scapy or Selenium can assist you with webscraping and parsing text from HTML and XML. \n",
        "\n",
        "You can run the example hereunder to see how a Webpage is scraped and parsed into tags, whihc can subsequently be questioned (remove the # before this line:\"pprint(soupified.prettify())\" to see the entire HTML file (it is long). "
      ],
      "metadata": {
        "id": "hgRVOWBFrjcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making the necessary imports,\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
        "html = urlopen(myurl).read() # query the website so that it returns a html page  \\n\"\n",
        "soupified = BeautifulSoup(html, 'html.parser') # parse the html in the 'html' variable, and store it in Beautiful Soup format\"\n",
        "\n",
        "#pprint(soupified.prettify())      # for printing the full HTML structure of the webpage\n",
        "\n",
        "question = soupified.find(\"div\", {\"class\": \"question\"}) # find the nevessary tag and class which it belongs to\n",
        "questiontext = question.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
        "print(\"Question: \\n\", questiontext.get_text().strip())\n",
        "answer = soupified.find(\"div\", {\"class\": \"answer\"}) # find the nevessary tag and class which it belongs to\n",
        "answertext = answer.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
        "print(\"Best answer: \\n\", answertext.get_text().strip())"
      ],
      "metadata": {
        "id": "bEmUPqnWrVx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f42b12-85d1-43be-f70d-ebcc57118796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: \n",
            " How do I get the current time?\n",
            "Best answer: \n",
            " Use datetime:\n",
            ">>> import datetime\n",
            ">>> now = datetime.datetime.now()\n",
            ">>> now\n",
            "datetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n",
            ">>> print(now)\n",
            "2009-01-06 15:08:24.789150\n",
            "\n",
            "For just the clock time without the date:\n",
            ">>> now.time()\n",
            "datetime.time(15, 8, 24, 78915)\n",
            ">>> print(now.time())\n",
            "15:08:24.789150\n",
            "\n",
            "\n",
            "To save typing, you can import the datetime object from the datetime module:\n",
            ">>> from datetime import datetime\n",
            "\n",
            "Then remove the prefix datetime. from all of the above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF can be quite challenging, especially from a formatting point of view. There are also many PDF reverse engineered formats that do not follow the official PDF guideliness completely. For popular formats from Microsoft, Google, Open Office and other vendors, there are several open source libraries to exract text and meta data. For more obscure file types, one has to fall back to commercial solutions such as Oracle Outside In, but these can be expensive. \n",
        "\n",
        "Encoding normalization is important to map various variants of code pages (https://en.wikipedia.org/wiki/Code_page ), ASCII and other encodings to one common Unicode format (https://home.unicode.org/). UTF-8 is the most used one. \n",
        "\n",
        "In this tutorial, we presume all this has been done and we can start with UTF-8 text files that only contain basic line (CR-LF) and tab formatting. "
      ],
      "metadata": {
        "id": "pboMwIeQwxil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK"
      ],
      "metadata": {
        "id": "fPfiHmSf7dmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we load NLTK"
      ],
      "metadata": {
        "id": "Tl18k70t1-EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # load tokenization "
      ],
      "metadata": {
        "id": "c44v65PO2AeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2d7c2f-8584-47b4-e42b-ffff60c1814f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK also contains many text corpora. Let's import the movie reviews."
      ],
      "metadata": {
        "id": "fD3v4P3lOZyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "movie_reviews.readme()"
      ],
      "metadata": {
        "id": "Gk1RSb6VOnDj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "159e7ce6-de10-404f-a685-14e16163fad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentiment Polarity Dataset Version 2.0\\nBo Pang and Lillian Lee\\n\\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\\n\\nDistributed with NLTK with permission from the authors.\\n\\n=======\\n\\nIntroduction\\n\\nThis README v2.0 (June, 2004) for the v2.0 polarity dataset comes from\\nthe URL http://www.cs.cornell.edu/people/pabo/movie-review-data .\\n\\n=======\\n\\nWhat\\'s New -- June, 2004\\n\\nThis dataset represents an enhancement of the review corpus v1.0\\ndescribed in README v1.1: it contains more reviews, and labels were\\ncreated with an improved rating-extraction system.\\n\\n=======\\n\\nCitation Info \\n\\nThis data was first used in Bo Pang and Lillian Lee,\\n``A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization \\nBased on Minimum Cuts\\'\\',  Proceedings of the ACL, 2004.\\n\\n@InProceedings{Pang+Lee:04a,\\n  author =       {Bo Pang and Lillian Lee},\\n  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\\n  booktitle =    \"Proceedings of the ACL\",\\n  year =         2004\\n}\\n\\n=======\\n\\nData Format Summary \\n\\n- review_polarity.tar.gz: contains this readme and  data used in\\n  the experiments described in Pang/Lee ACL 2004.\\n\\n  Specifically:\\n\\n  Within the folder \"txt_sentoken\" are the 2000 processed down-cased\\n  text files used in Pang/Lee ACL 2004; the names of the two\\n  subdirectories in that folder, \"pos\" and \"neg\", indicate the true\\n  classification (sentiment) of the component files according to our\\n  automatic rating classifier (see section \"Rating Decision\" below).\\n\\n  File names consist of a cross-validation tag plus the name of the\\n  original html file.  The ten folds used in the Pang/Lee ACL 2004 paper\\'s\\n  experiments were:\\n\\n     fold 1: files tagged cv000 through cv099, in numerical order\\n     fold 2: files tagged cv100 through cv199, in numerical order     \\n     ...\\n     fold 10: files tagged cv900 through cv999, in numerical order\\n\\n  Hence, the file neg/cv114_19501.txt, for example, was labeled as\\n  negative, served as a member of fold 2, and was extracted from the\\n  file 19501.html in polarity_html.zip (see below).\\n\\n  Each line in each text file corresponds to a single sentence, as\\n  determined by Adwait Ratnaparkhi\\'s sentence boundary detector\\n  MXTERMINATOR.\\n \\n  Preliminary steps were taken to remove rating information from the\\n  text files, but only the rating information upon which the rating\\n  decision was based is guaranteed to have been removed. Thus, if the\\n  original review contains several instances of rating information,\\n  potentially given in different forms, those not recognized as valid\\n  ratings remain part of the review text.\\n\\t\\n- polarity_html.zip: The original source files from which the\\n  processed, labeled, and (randomly) selected data in\\n  review_polarity.tar.gz was derived.\\n\\n  Specifically:  \\n\\n  This data consists of unprocessed, unlabeled html files from the\\n  IMDb archive of the rec.arts.movies.reviews newsgroup,\\n  http://reviews.imdb.com/Reviews. The files in review_polarity.tar.gz\\n  represent a processed subset of these files. \\n\\n=======\\n\\nRating Decision (Appendix A)\\n\\nThis section describes how we determined whether a review was positive\\nor negative.\\n\\nThe original html files do not have consistent formats -- a review may\\nnot have the author\\'s rating with it, and when it does, the rating can\\nappear at different places in the file in different forms.  We only\\nrecognize some of the more explicit ratings, which are extracted via a\\nset of ad-hoc rules.  In essence, a file\\'s classification is determined\\nbased on the first rating we were able to identify.\\n\\n\\n- In order to obtain more accurate rating decisions, the maximum\\n\\trating must be specified explicitly, both for numerical ratings\\n\\tand star ratings.  (\"8/10\", \"four out of five\", and \"OUT OF\\n\\t****: ***\" are examples of rating indications we recognize.)\\n\\n- With a five-star system (or compatible number systems):\\n\\tthree-and-a-half stars and up are considered positive, \\n\\ttwo stars and below are considered negative.\\n- With a four-star system (or compatible number system):\\n\\tthree stars and up are considered positive, \\n\\tone-and-a-half stars and below are considered negative.  \\n- With a letter grade system:\\n\\tB or above is considered positive,\\n\\tC- or below is considered negative.\\n\\nWe attempted to recognize half stars, but they are specified in an\\nespecially free way, which makes them difficult to recognize.  Hence,\\nwe may lose a half star very occasionally; but this only results in 2.5\\nstars in five star system being categorized as negative, which is \\nstill reasonable.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what is in there"
      ],
      "metadata": {
        "id": "XVqQvmAZPDeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw = movie_reviews.raw()\n",
        "print(raw[0:1000:1]) # print first 1000 chars\n"
      ],
      "metadata": {
        "id": "POr_DqyYPFKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2062fe-c57d-4d6e-cd83-8701423013b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plot : two teen couples go to a church party , drink and then drive . \n",
            "they get into an accident . \n",
            "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
            "what's the deal ? \n",
            "watch the movie and \" sorta \" find out . . . \n",
            "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
            "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
            "they seem to have taken this pretty neat concept , but executed it terribly . \n",
            "so what are the problems with the movie ? \n",
            "well , its main problem is that it's simply too jumbled . \n",
            "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can detect the long tail that is typical for natural language. First we seperate the text in indivudual words, then we run a frequency analsyis on the results. "
      ],
      "metadata": {
        "id": "EdCLI7XPPzLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = movie_reviews.words()\n",
        "print(corpus)\n",
        "freq_dist = nltk.FreqDist(corpus)\n",
        "print(freq_dist)\n",
        "print(freq_dist.most_common(50))\n",
        "freq_dist.plot(500)"
      ],
      "metadata": {
        "id": "q-iCAp-ZP3ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: \n",
        "So, what do you observe? What are the most frequent words?"
      ],
      "metadata": {
        "id": "ID49Xu16QW8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "v4Tq_8i-Ut3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can also observe, punctuation characters such as .  and , and other ones (: ; \" \" ? ! ) are still in there. This is where tokenization comes in. Tokenization removes punctuations where they are used as sentence and phrase seperation, but leaves them where they are part of a token (e.g. an email address or abbreviation).  "
      ],
      "metadata": {
        "id": "ApQiVxWzQgqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentence Detection"
      ],
      "metadata": {
        "id": "7AfVoIBc7gh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we load the NLTK tokenizer for sentences (sent_tokenize) and for words (word_tokenize)"
      ],
      "metadata": {
        "id": "L2oTaS-W2KuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "y9gnIhNd2JDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"The Department of Advanced Computing Sciences - sometimes abbreviated as DACS - \\n is Maastricht University’s largest and oldest department \\n broadly covering the fields of artificial intelligence, data science, computer science, \\n mathematics and robotics. We maintain a large network of public and \\n private partners through our research collaborations and through the \\n award-winning KE@Work programme. In addition, our staff teaches approximately 800 bachelor’s and master’s \\n students in 3 specialized study programmes in Data Science \\n and Artificial Intelligence. The Department of Advanced Computing Sciences \\n  is the new joint identity of the Institute of Data Science (IDS) and the former \\n Department of Data Science and Knowledge Engineering (DKE).\"\n",
        "print(my_text)\n"
      ],
      "metadata": {
        "id": "T6CLqSUA29BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_sentences = sent_tokenize(my_text)\n",
        "# print(my_sentences) # print entire list unformatted\n",
        "print(\"\\n\")\n",
        "for x in range(len(my_sentences)):\n",
        "    print(my_sentences[x]+\"\\n\")"
      ],
      "metadata": {
        "id": "P48pIjx8XW2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Detection aka Tokenization"
      ],
      "metadata": {
        "id": "MA5suVwx7lHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n",
        "A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence. "
      ],
      "metadata": {
        "id": "LqK3BNkLX5X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in my_sentences:\n",
        "    print(\"Sentence: \"+str(sentence))\n",
        "    my_words = word_tokenize(sentence)\n",
        "    print(\"Tokens: \")\n",
        "    for x in range(len(my_words)):\n",
        "      print(\"    \"+str(my_words[x]))\n"
      ],
      "metadata": {
        "id": "ISHpq9JAY4XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can observe, there are still punctuation in the list of tokens. In NLTK these can be removed by using a regular expression."
      ],
      "metadata": {
        "id": "tnvfN0OsairB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "new_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for sentence in my_sentences:\n",
        "    print(\"Sentence: \"+str(sentence))\n",
        "    my_words = new_tokenizer.tokenize(sentence)\n",
        "    print(\"Tokens: \")\n",
        "    for x in range(len(my_words)):\n",
        "      print(\"    \"+str(my_words[x]))"
      ],
      "metadata": {
        "id": "uERolN74aqXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2: \n",
        "As you can observe, our tokenizer cannot deal with tokens such as \"Bachelor's\" or \"Master's\". These are tokenized in Bachelor + s and Master + s. Describe hereunder how we could solve that. Also include the code."
      ],
      "metadata": {
        "id": "YhgIu4uKcbmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "oL39SsVJc3QQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a Vocabulary"
      ],
      "metadata": {
        "id": "B-OCi_nxIPYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vocabulary is a data structure containing every unique word used in the corpus only once and in alphabetical order. This can be used as a dictionairy in NLP or as the basis of a search index in information retrieval. "
      ],
      "metadata": {
        "id": "STaUFJPJJIrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tokens = new_tokenizer.tokenize(my_text.lower()) #use the tokenizer that removes punctuation\n",
        "vocab = sorted(set(corpus_tokens))\n",
        "print(vocab)\n",
        "print(\"Tokens:\", len(corpus_tokens))\n",
        "print(\"Vocabulary:\", len(vocab))"
      ],
      "metadata": {
        "id": "guS6cWr6ISTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stop Words"
      ],
      "metadata": {
        "id": "51E04B3g7pz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the past, when computer resources were still limited, highly frequent words were often removed in information retrieval applications. These are named stop-words or noise-words. These are words such as \"the, on, in, a, be, or, and, an, for, to, ...\". If such a word is removed, one can no longer search for them. Imagine searching for \"to be or not to be\", which is no longer after noise word removal.\n",
        "\n",
        "In text-mining and advanced NLP, these words often contain important clues on the meaning of language. \n",
        "\n",
        "So, these days as computr resources are much larger, noise words are more often not removed. \n",
        "\n",
        "But let's try how to remoce them using NLTK. "
      ],
      "metadata": {
        "id": "JxnKcmMt4tDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(\"Stopwords from NLTK:\", stopwords.words('english'))\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "# we use the token list without punctuations\n",
        "print(\"Tokenized corpus:\",corpus_tokens)\n",
        "#now remove stopwords\n",
        "tokenized_corpus_without_stopwords = [i for i in corpus_tokens if not i in stop_words_nltk]\n",
        "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)"
      ],
      "metadata": {
        "id": "95Xp0Zlc5qOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3:\n",
        "\n",
        "What do you observe with respect to case sensitivity? \n",
        "How can we solve that? \n",
        "Include the revised code\n",
        "Could this actions also lead to unwanted side effects?\n",
        "\n",
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "_7bgF2uT7P0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming"
      ],
      "metadata": {
        "id": "jsjAz4lF7Y0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process of removing suffixes and reducing the word to some base form such that all different variations of a word can be represented by one form. Stemming uses rules and may not always result in the correct linguistic base form. However, it is fast and therefor often used by search engines. As we discussed in the lecture, a well-known stemmer for the English language is the Porter stemmer. \n",
        "\n",
        "Let's try it ..."
      ],
      "metadata": {
        "id": "6UYjccj-RvK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer= PorterStemmer()\n",
        "print(\"before stemming -> after stemming\")\n",
        "for word in corpus_tokens:\n",
        "  print(str(word) + \" -> \" + str(stemmer.stem(word)))\n",
        "  "
      ],
      "metadata": {
        "id": "-o3TscOrSXGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, \"students\" is converted into \"student\", but \"Science\" is converted into \"scien\". There are other non-linguistically correct transformations. "
      ],
      "metadata": {
        "id": "6XnibDb_Xs1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "EDc2SVqq7w2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This why we prefer to use lemmatization for linguistic applications other than search engines. Lemmatization is the process of mapping all tokens to its base-linguistic form: the \"lemma\". So \"better\" should be converted to \"good\" and \"is\" to \"be\". "
      ],
      "metadata": {
        "id": "Kav2cXbpYKa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # downloading wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "print(\"before lemmatization -> after lemmarization\")\n",
        "for word in corpus_tokens:\n",
        "  print(str(word) + \" -> \" + str(lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "id": "zv5796gKY8C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can observe, only plurals and other basic operations are performed. But \"is\" not converted to \"be\". Neither are several verb inflections. This is because Lemmatization requires more linguistic knowledge: it need to know whether we are dealing with, for instance, a verb, noun or a adjectice. We call these gramatical roles \"part-of-speech\" or POS tags. These will be discussed in the next lecture: Syntax and Semantics."
      ],
      "metadata": {
        "id": "t4l3z9ajbeIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('better'))\n",
        "print(lemmatizer.lemmatize('better',pos='a')) # a for Adjective\n",
        "print(lemmatizer.lemmatize('is'))\n",
        "print(lemmatizer.lemmatize('is',pos='v'))  # v for Verb\n",
        "print(lemmatizer.lemmatize('is',pos='a'))\n",
        "print(lemmatizer.lemmatize('is',pos='n'))  # n for Noun\n",
        "print(lemmatizer.lemmatize('richer',pos='n'))\n",
        "print(lemmatizer.lemmatize('richer',pos='a'))    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F9Cq2Xx2b4I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-Normalization"
      ],
      "metadata": {
        "id": "QE6ntyME72S0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In social media, one can run in short-cuts, slang, hash-tags, or emoticons. These can be concerted to their textual forms. Phone numbers, dates and monataire amounts can be written in many different forms. Sometimes, one can even decide to convert all text to either lower case or upper case. This may cause problems in some applications and should be used carefully. We will discuss this in more detail in the course Text Mining, where this is more important. "
      ],
      "metadata": {
        "id": "eyicUNL2enhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Detection"
      ],
      "metadata": {
        "id": "NeGSKRnx78Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all NLP models and algorithms are very language specific: this means that one can only use them with the intenred language. Using them on other language will result in random behavior.  \n",
        "\n",
        "So, language detection (often per sentence or minimally per paragrpah) is essential for any type of NLP application to perform correctly!"
      ],
      "metadata": {
        "id": "IhONHqV2fiU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "from langdetect import detect, detect_langs\n",
        "def language_detection(text, method = \"single\"):\n",
        "  if(method.lower() != \"single\"):\n",
        "    result = detect_langs(text)\n",
        "  else:\n",
        "    result = detect(text)\n",
        "  return result\n",
        "\n",
        "multilingual_text = \"Elle est vraiment éfficace dans la détection de langue.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Het is enorm makkelijk om een taal te herkennen!\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Es ist wirklich effektiv bei der Spracherkennung.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Nó thực sự hiệu quả trong việc phát hiện ngôn ngữ.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"إنه فعال حقًا في اكتشاف اللغة.\"\n",
        "print(language_detection(multilingual_text))"
      ],
      "metadata": {
        "id": "njzaEIDBgFAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transliteration"
      ],
      "metadata": {
        "id": "OmCN9E977_mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transliteration refers to the method of mapping from one system of writing to another based on phonetic similarity. With this tool, you type in Latin letters (e.g. a, b, c etc.), which are converted to characters that have similar pronunciation in the target language. For transliteration, you need to select the target language. So, results for a transliteration of a Arabic name into English, French or German can be very different for similar names. "
      ],
      "metadata": {
        "id": "munkN882hiya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лев Николаевич Толстой\n",
        "\n",
        "results in different forms of transliteration for different target languages:\n",
        "\n",
        "Lev Nikolayevich Tolstoy\n",
        "\n",
        "Léon Tolstoï\n",
        "\n",
        "Lev Tolstoj\n",
        "\n",
        "León Tolstó\n",
        "\n",
        "Lev Tolstoy\n",
        "\n",
        "Lav Tolstoj \n",
        "\n",
        "Lev Tolsto\n",
        "\n",
        "Liuni Tolstoi\n",
        "\n",
        "Ļevs Tolstojs\n",
        "\n",
        "Levs Tuolstuos\n",
        "\n",
        "...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ciweSsXiu0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Python library for transliteration can be found here: https://pypi.org/project/transliterate/. We will discuss this in more detail in the lecture on Machine Translation."
      ],
      "metadata": {
        "id": "QvS5d3LEiNcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4: Any final reflextions on tokenization?"
      ],
      "metadata": {
        "id": "vV7f89o0VYzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "5ol5sN4fVfdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "Please share your Colab notebook by clicking File on the top-left corner. Click under Download on Download .ipynb and upload that file to Canvas."
      ],
      "metadata": {
        "id": "ZdS9JFQ5UXkX"
      }
    }
  ]
}